package storage

import (
	"encoding/json"
	"errors"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"time"
	"work_2/internal/core/block"
	"work_2/internal/core/metrics"
	"work_2/internal/core/skiplist"

	lru "github.com/hashicorp/golang-lru/v2"

	"github.com/syndtr/goleveldb/leveldb"
)

const (
	ChunkSizeThreshold = 2 * 1024 * 1024 // 2MB
	CacheSize          = 128             // ç¼“å­˜128ä¸ªæœ€è¿‘è®¿é—®çš„chunk
	BlockCacheSize     = 1024            // é¢å¤–ç¼“å­˜1024ä¸ªå•ç‹¬çš„åŒºå—
	TxCacheSize        = 2048            // é¢å¤–ç¼“å­˜2048ä¸ªäº¤æ˜“
	UserHotThreshold   = 10              // è®¾å®šæŸ¥è¯¢æ¬¡æ•°è¶…è¿‡10æ¬¡çš„ç”¨æˆ·ä¸ºçƒ­ç”¨æˆ·
)

// UserProfile ç»“æ„ä½“
type UserProfile struct {
	UserID        string
	QueryCount    int             // æ€»æŸ¥è¯¢æ¬¡æ•°ï¼ˆå†å²ç´¯è®¡ï¼‰
	LastQueryTime time.Time       // æœ€è¿‘ä¸€æ¬¡æŸ¥è¯¢æ—¶é—´
	QueryHistory  map[string]int  // æ»‘åŠ¨çª—å£ï¼ˆå°æ—¶ç»Ÿè®¡ï¼‰
	DailyPattern  map[int]float64 // è¿‡å» 7 å¤©æ¯å°æ—¶å¹³å‡æŸ¥è¯¢æ¬¡æ•°ï¼ˆSMAï¼‰
	QuerySequence []string        // è®°å½•ç”¨æˆ·æœ€è¿‘æŸ¥è¯¢çš„äº¤æ˜“ ID
}

// **ç¼“å­˜äº¤æ˜“å’Œæ‰€åœ¨åŒºå—**
type TxCacheEntry struct {
	Tx    *block.Transaction
	Block *block.Block
}

// ä»äº¤æ˜“ ID åˆ°å®ƒæ‰€åœ¨åŒºå—å·ä»¥åŠåœ¨åŒºå—å†…çš„ç´¢å¼•çš„æ˜ å°„ flushçš„æ—¶å€™ç”¨åˆ°
// å³ä¿å­˜äº¤æ˜“åœ¨ Chunk å†…çš„ä½ç½®ä¿¡æ¯
type TxOffset struct {
	BlockNumber uint64 // äº¤æ˜“æ‰€åœ¨çš„åŒºå—å·
	TxIndex     int    // åœ¨è¯¥åŒºå— Transactions æ•°ç»„ä¸­çš„ç´¢å¼•
}

type ChunkStorage struct {
	db         *leveldb.DB
	path       string
	mu         sync.RWMutex
	currentBuf []*block.Block //å½“å‰å†…å­˜ç¼“å†²åŒº
	chunkIDSeq uint64
	cache      *lru.Cache[uint64, *Chunk]        // ä½¿ç”¨LRUç¼“å­˜æœ€è¿‘è®¿é—®çš„chunk
	blockCache *lru.Cache[uint64, *block.Block]  // LRUç¼“å­˜æœ€è¿‘è®¿é—®çš„Block
	txCache    *lru.Cache[string, *TxCacheEntry] // ç¼“å­˜äº¤æ˜“
	skipList   *skiplist.SkipList                // è·³è¡¨
}

type Chunk struct {
	ID         uint64                  `json:"id"`
	StartBlock uint64                  `json:"start"`      // åŒ…å«çš„èµ·å§‹åŒºå—å·
	EndBlock   uint64                  `json:"end"`        // ç»“æŸåŒºå—å·
	CreatedAt  int64                   `json:"created"`    // æ—¶é—´æˆ³
	Data       []byte                  `json:"data"`       // åºåˆ—åŒ–çš„åŒºå—æ•°æ®
	Index      map[uint64]int          `json:"index"`      // åŒºå—å·->æ•°æ®åç§»é‡
	Blocks     map[uint64]*block.Block `json:"blocks"`     // å­˜å‚¨æ¯ä¸ªåŒºå— JSON
	TxOffsets  map[string]TxOffset     `json:"tx_offsets"` // äº¤æ˜“ç´¢å¼•æ˜ å°„
}

// åˆå§‹åŒ–å­˜å‚¨ç³»ç»Ÿ
func NewChunkStorage(path string) (*ChunkStorage, error) {
	fullPath := filepath.Join(path, "chunks")
	db, err := leveldb.OpenFile(fullPath, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to open leveldb: %v", err)
	}

	chunkCache, err := lru.New[uint64, *Chunk](CacheSize)
	if err != nil {
		db.Close()
		return nil, fmt.Errorf("failed to create chunk cache: %v", err)
	}

	blockCache, err := lru.New[uint64, *block.Block](BlockCacheSize)
	if err != nil {
		db.Close()
		return nil, fmt.Errorf("failed to create block cache: %v", err)
	}

	txCache, err := lru.New[string, *TxCacheEntry](TxCacheSize)
	if err != nil {
		db.Close()
		return nil, fmt.Errorf("failed to create tx cache: %v", err)
	}

	return &ChunkStorage{
		db:         db,
		path:       fullPath,
		currentBuf: make([]*block.Block, 0),
		chunkIDSeq: 0,
		cache:      chunkCache,
		blockCache: blockCache,
		txCache:    txCache,
		skipList:   skiplist.NewSkipList(16, 0.5),
	}, nil
}

// ======================== æ ¸å¿ƒå­˜å‚¨é€»è¾‘ ========================
func (cs *ChunkStorage) AddBlock(b *block.Block) error {
	cs.mu.Lock()
	defer cs.mu.Unlock()

	cs.currentBuf = append(cs.currentBuf, b)

	if cs.shouldFlush() {
		return cs.Flush()
	}
	return nil
}

func (cs *ChunkStorage) shouldFlush() bool {
	totalSize := 0
	for _, b := range cs.currentBuf {
		totalSize += b.Size()
	}
	return totalSize >= ChunkSizeThreshold
}

// å°†ç¼“å†²åŒºæ•°æ®æŒä¹…åŒ–ä¸ºChunk
// 1ã€åˆ›å»ºChunkç»“æ„å¹¶å¡«å……å…ƒæ•°æ®
// 2ã€éå†ç¼“å†²åŒºä¸­çš„åŒºå—ï¼š
//
//	å»ºç«‹åŒºå—å·åˆ°ChunkIDçš„ç´¢å¼•
//	æ„å»ºäº¤æ˜“IDåˆ°åŒºå—ä½ç½®çš„æ˜ å°„
//
// 3ã€åºåˆ—åŒ–Chunkå¹¶å­˜å…¥LevelDB
// 4ã€æ›´æ–°chunkIDSeqåºåˆ—å·
// 5ã€æ¸…ç©ºcurrentBuf
func (cs *ChunkStorage) Flush() error {
	cs.mu.Lock()
	defer cs.mu.Unlock()

	if len(cs.currentBuf) == 0 {
		return nil
	}

	chunk := Chunk{
		ID:         cs.chunkIDSeq,
		StartBlock: cs.currentBuf[0].Number,
		EndBlock:   cs.currentBuf[len(cs.currentBuf)-1].Number,
		CreatedAt:  time.Now().Unix(),
		Blocks:     make(map[uint64]*block.Block),
		TxOffsets:  make(map[string]TxOffset),
	}

	for _, b := range cs.currentBuf {
		chunk.Blocks[b.Number] = b
		blockIndexKey := []byte(fmt.Sprintf("block_index:%d", b.Number))
		cs.db.Put(blockIndexKey, []byte(fmt.Sprintf("%d", chunk.ID)), nil)

		// éå†åŒºå—ä¸­çš„äº¤æ˜“ï¼Œæ„å»ºäº¤æ˜“ç´¢å¼•
		for i, tx := range b.Transactions {
			// å­˜å‚¨ LevelDB çš„ tx_index ä»å¯ä»¥ä¿ç•™ï¼Œä½†è¿™é‡Œæˆ‘ä»¬åŒæ—¶è®°å½•æ›´è¯¦ç»†çš„ä¿¡æ¯åˆ° chunk.TxOffsets
			txKey := []byte(fmt.Sprintf("tx_index:%s", tx.TXID))
			cs.db.Put(txKey, []byte(fmt.Sprintf("%d", chunk.ID)), nil)

			chunk.TxOffsets[tx.TXID] = TxOffset{
				BlockNumber: b.Number,
				TxIndex:     i,
			}
		}
	}

	chunkData, err := json.Marshal(chunk)
	if err != nil {
		return fmt.Errorf("marshal chunk failed: %w", err)
	}

	chunkKey := []byte(fmt.Sprintf("chunk:%d", chunk.ID))
	if err := cs.db.Put(chunkKey, chunkData, nil); err != nil {
		return fmt.Errorf("store chunk failed: %w", err)
	}

	cs.currentBuf = nil
	cs.chunkIDSeq++
	return nil
}

// ======================== æŸ¥è¯¢æ¥å£ ========================
// GetChunkIDSeq è¿”å›å½“å‰çš„ chunkIDSeq å€¼
func (cs *ChunkStorage) GetChunkIDSeq() uint64 {
	return cs.chunkIDSeq
}

// GetDB è¿”å›å½“å‰çš„æ•°æ®åº“å®ä¾‹
func (cs *ChunkStorage) GetDB() *leveldb.DB {
	return cs.db
}

// æ·»åŠ ä»¥ä¸‹æ–¹æ³•åˆ° ChunkStorage ç»“æ„ä½“

// GetChunkCount è·å–å·²ç”Ÿæˆçš„å¤§å—æ€»æ•°
func (cs *ChunkStorage) GetChunkCount() uint64 {
	cs.mu.Lock()
	defer cs.mu.Unlock()
	return cs.chunkIDSeq
}

// GetBlockByNumber é€šè¿‡åŒºå—å·æŸ¥è¯¢åŸå§‹æ•°æ®
func (cs *ChunkStorage) GetBlockByNumber(num uint64) ([]byte, error) {
	cs.mu.RLock()
	defer cs.mu.RUnlock()

	metrics.LevelDBReadOps.Inc()
	// **0. ä¼˜å…ˆæŸ¥è¯¢è·³è¡¨ï¼ˆçƒ­ç‚¹åŒºå—ï¼‰**
	if node := cs.skipList.Search(num); node != nil {
		// è·³è¡¨å‘½ä¸­ï¼Œè¿”å›è¯¥èŠ‚ç‚¹å­˜å‚¨çš„åŒºå—
		return json.Marshal(node.Block)
	}

	// **1. å…ˆæŸ¥ blockCache**
	if block, ok := cs.blockCache.Get(num); ok {
		// åŒæ—¶å°†æŸ¥åˆ°çš„ Block æ’å…¥è·³è¡¨ï¼Œä»¥ä¾¿ä¸‹æ¬¡ç›´æ¥å‘½ä¸­
		cs.skipList.Insert(num, block)
		return json.Marshal(block)
	}

	// **2. æŸ¥è¯¢LevelDBçš„åŒºå—ç´¢å¼•è·å–ChunkID**
	indexKey := []byte(fmt.Sprintf("block_index:%d", num))
	chunkIDBytes, err := cs.db.Get(indexKey, nil)
	if err != nil {
		return nil, fmt.Errorf("block index lookup failed: %w", err)
	}
	chunkID, _ := strconv.ParseUint(string(chunkIDBytes), 10, 64)

	// **3. åŠ è½½ Chunk**
	chunk, err := cs.loadChunk(chunkID)
	if err != nil {
		return nil, err
	}

	// **4. æå– Block å¹¶ç¼“å­˜**
	block, exists := chunk.Blocks[num]
	if !exists {
		return nil, errors.New("block not found in chunk")
	}
	cs.blockCache.Add(num, block) // åŠ å…¥ block ç¼“å­˜
	// åŒæ—¶å°†è¯¥ Block æ’å…¥è·³è¡¨
	cs.skipList.Insert(num, block)

	return json.Marshal(block)
}

// é€šè¿‡äº¤æ˜“ç´¢å¼•æŸ¥æ‰¾
func (cs *ChunkStorage) GetBlockByTxHash(txHash string) ([]byte, []byte, error) {
	cs.mu.RLock()
	defer cs.mu.RUnlock()

	// **1. æŸ¥è¯¢ TxCacheï¼ˆLRU ç¼“å­˜ï¼‰**
	if entry, ok := cs.txCache.Get(txHash); ok {
		cs.UpdateUserProfile(entry.Tx.From, txHash)

		blockData, err := json.Marshal(entry.Block)
		if err != nil {
			return nil, nil, fmt.Errorf("block marshal failed: %w", err)
		}
		txData, err := json.Marshal(entry.Tx)
		if err != nil {
			return nil, nil, fmt.Errorf("tx marshal failed: %w", err)
		}
		return blockData, txData, nil
	}

	// **2. æŸ¥è¯¢ LevelDB è·å– ChunkID**
	txKey := []byte(fmt.Sprintf("tx_index:%s", txHash))
	chunkIDBytes, err := cs.db.Get(txKey, nil)
	if err != nil {
		return nil, nil, fmt.Errorf("tx index lookup failed: %w", err)
	}
	chunkID, _ := strconv.ParseUint(string(chunkIDBytes), 10, 64)

	// **3. åŠ è½½ Chunk**
	chunk, err := cs.loadChunk(chunkID)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to load chunk: %w", err)
	}

	// **4. åˆ©ç”¨ Chunk å†…çš„äº¤æ˜“åç§»ç´¢å¼•ï¼Œç›´æ¥æŸ¥æ‰¾äº¤æ˜“**
	offset, ok := chunk.TxOffsets[txHash]
	if !ok {
		return nil, nil, errors.New("transaction not found in tx_offsets")
	}

	// **5. å…ˆæŸ¥è¯¢è·³è¡¨ï¼Œçœ‹çœ‹æ˜¯å¦å·²ç»ç¼“å­˜äº†è¿™ä¸ªåŒºå—**
	if node := cs.skipList.Search(offset.BlockNumber); node != nil {
		// **è·³è¡¨å‘½ä¸­**
		blockData, err := json.Marshal(node.Block)
		if err != nil {
			return nil, nil, fmt.Errorf("block marshal failed: %w", err)
		}
		tx := node.Block.Transactions[offset.TxIndex]
		txData, err := json.Marshal(tx)
		if err != nil {
			return nil, nil, fmt.Errorf("tx marshal failed: %w", err)
		}
		// **ç¼“å­˜äº¤æ˜“**
		cs.txCache.Add(txHash, &TxCacheEntry{Tx: &tx, Block: node.Block})
		return blockData, txData, nil
	}

	// **6. è·³è¡¨æœªå‘½ä¸­ï¼Œä» Chunk åŠ è½½åŒºå—**
	targetBlock, exists := chunk.Blocks[offset.BlockNumber]
	if !exists {
		return nil, nil, errors.New("block not found in chunk")
	}

	// **7. æ ¡éªŒäº¤æ˜“ç´¢å¼•**
	if offset.TxIndex < 0 || offset.TxIndex >= len(targetBlock.Transactions) {
		return nil, nil, errors.New("invalid transaction index")
	}
	tx := targetBlock.Transactions[offset.TxIndex]

	// **8. ç¼“å­˜æŸ¥æ‰¾åˆ°çš„äº¤æ˜“ & åŒºå—**
	cs.txCache.Add(txHash, &TxCacheEntry{Tx: &tx, Block: targetBlock})
	cs.skipList.Insert(targetBlock.Number, targetBlock) // **æ’å…¥è·³è¡¨**

	// **9. æ›´æ–°ç”¨æˆ·æŸ¥è¯¢è¡Œä¸º**
	cs.UpdateUserProfile(tx.From, txHash)

	// **10. è§¦å‘ Lookahead é¢„åŠ è½½**
	if cs.IsHotUser(tx.From) || cs.IsCyclicHotUser(tx.From) {
		cs.PreloadUserTransactions(tx.From)
	}

	// **11. é¢„æµ‹ç”¨æˆ·ä¸‹ä¸€æ¬¡å¯èƒ½æŸ¥è¯¢çš„äº¤æ˜“**
	if predictedTx := cs.PredictNextQuery(tx.From); predictedTx != "" {
		fmt.Printf("é¢„æµ‹ç”¨æˆ· %s å¯èƒ½æŸ¥è¯¢: %s\n", tx.From, predictedTx)
		cs.PreloadUserTransactions(predictedTx) // é¢„åŠ è½½é¢„æµ‹çš„äº¤æ˜“
	}

	// **12. è¿”å›äº¤æ˜“ & åŒºå—æ•°æ®**
	blockData, _ := json.Marshal(targetBlock)
	txData, _ := json.Marshal(tx)
	return blockData, txData, nil
}

// **åŠ è½½ Chunkï¼Œå¹¶å°† Block é€ä¸ªå­˜å…¥ç¼“å­˜ ç¼“å­˜äº¤æ˜“**
func (cs *ChunkStorage) loadChunk(chunkID uint64) (*Chunk, error) {
	// å…ˆæ£€æŸ¥ç¼“å­˜ä¸­çš„ Chunk
	if cachedChunk, ok := cs.cache.Get(chunkID); ok {
		return cachedChunk, nil
	}

	chunkKey := []byte(fmt.Sprintf("chunk:%d", chunkID))
	chunkData, err := cs.db.Get(chunkKey, nil)
	if err != nil {
		log.Printf("æŸ¥è¯¢ chunk å¤±è´¥: %v", err)
		return nil, fmt.Errorf("chunk not found: %w", err)
	}

	var chunk Chunk
	if err := json.Unmarshal(chunkData, &chunk); err != nil {
		log.Printf("è§£æ chunk å¤±è´¥: %v", err)
		return nil, fmt.Errorf("chunk unmarshal failed: %w", err)
	}

	// ç¼“å­˜åŠ è½½çš„ Chunk
	cs.cache.Add(chunkID, &chunk)

	// ç¼“å­˜ Chunk ä¸­æ‰€æœ‰ Block
	for blockNum, blk := range chunk.Blocks {
		cs.blockCache.Add(blockNum, blk)
		cs.skipList.Insert(blockNum, blk)
	}

	// ä¸å†éå†å¹¶ç¼“å­˜æ¯ä¸ªäº¤æ˜“åˆ° txCacheï¼ŒæŸ¥è¯¢æ—¶ç›´æ¥åˆ©ç”¨ TxOffsets å®šä½äº¤æ˜“

	return &chunk, nil
}

// ======================== å·¥å…·æ–¹æ³• ========================
// Close é‡Šæ”¾æ•°æ®åº“èµ„æº
func (cs *ChunkStorage) Close() error {
	return cs.db.Close()
}

func (cs *ChunkStorage) WriteChunk(data []byte) error {
	return cs.db.Put([]byte("chunk0"), data, nil)
}

// internal/core/storage/chunk.go
func (cs *ChunkStorage) WriteBlock(b *block.Block) error {
	data, err := json.Marshal(b)
	if err != nil {
		return err
	}
	key := []byte(fmt.Sprintf("block:%d", b.Number))
	fmt.Printf("æ­£åœ¨å†™å…¥åŒºå— %d â†’ å¤§å° %d å­—èŠ‚\n", b.Number, len(data)) // æ·»åŠ æ—¥å¿—
	return cs.db.Put(key, data, nil)
}

func (cs *ChunkStorage) DBPath() string {
	return cs.path
}

// æ‰¹é‡å†™å…¥æ–¹æ³•
func (cs *ChunkStorage) WriteBlocks(blocks []*block.Block) error {
	cs.mu.Lock()
	defer cs.mu.Unlock()

	// åˆå¹¶åˆ°å½“å‰ç¼“å†²åŒº
	cs.currentBuf = append(cs.currentBuf, blocks...)

	if cs.shouldFlush() {
		return cs.Flush()
	}
	return nil
}

// æµ‹è¯•æ•°æ®åŠ è½½æ–¹æ³•
func LoadTestBlocksFromDir(dir string) ([]*block.Block, error) {
	files, _ := os.ReadDir(dir)
	var blocks []*block.Block

	for _, f := range files {
		data, _ := os.ReadFile(filepath.Join(dir, f.Name()))
		var b block.Block
		if err := json.Unmarshal(data, &b); err == nil {
			blocks = append(blocks, &b)
		}
	}
	return blocks, nil
}

// æ›´æ–°ç”¨æˆ·æŸ¥è¯¢è¡Œä¸º
func (cs *ChunkStorage) UpdateUserProfile(userID, txHash string) {
	key := []byte(fmt.Sprintf("user_profile:%s", userID))

	data, err := cs.db.Get(key, nil)
	var profile UserProfile
	if err == nil {
		_ = json.Unmarshal(data, &profile)
	} else {
		profile = UserProfile{
			UserID:        userID,
			QueryCount:    0,
			QueryHistory:  make(map[string]int),
			DailyPattern:  make(map[int]float64),
			QuerySequence: []string{},
		}
	}

	// **è®°å½•å½“å‰å°æ—¶æŸ¥è¯¢æ¬¡æ•°**
	currentHour := time.Now().Format("2006-01-02 15")
	profile.QueryHistory[currentHour]++
	profile.QueryCount++
	profile.LastQueryTime = time.Now()

	// **ç§»é™¤è¶…å‡ºæ»‘åŠ¨çª—å£èŒƒå›´çš„æ•°æ®**
	windowSize := 24 // 24 å°æ—¶
	profile = pruneOldHistory(profile, windowSize)

	// **è®¡ç®—è¿‡å» 7 å¤©çš„ SMA è¯†åˆ«å‘¨æœŸæ€§æŸ¥è¯¢**
	profile = computeDailyPattern(profile, 7)

	// **æ›´æ–°ç”¨æˆ·æŸ¥è¯¢åºåˆ—ï¼ˆæœ€å¤šå­˜ 10 æ¬¡ï¼ŒFIFO æ–¹å¼ï¼‰**
	profile.QuerySequence = append(profile.QuerySequence, txHash)
	if len(profile.QuerySequence) > 10 {
		profile.QuerySequence = profile.QuerySequence[1:] // åªä¿ç•™æœ€è¿‘ 10 æ¡æŸ¥è¯¢
	}

	// **å­˜å…¥ LevelDB**
	data, _ = json.Marshal(profile)
	_ = cs.db.Put(key, data, nil)

	// **è°ƒç”¨é©¬å°”å¯å¤«é¢„æµ‹ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½é¢„æµ‹ä¸‹ä¸€ä¸ªæŸ¥è¯¢**
	if predictedTx := cs.PredictNextQuery(userID); predictedTx != "" {
		fmt.Printf("ğŸ”® é¢„æµ‹ç”¨æˆ· %s å¯èƒ½æŸ¥è¯¢: %s\n", userID, predictedTx)
	}
}

// è®¡ç®—å‘¨æœŸæ€§æŸ¥è¯¢æ¨¡å¼ - SMA
func computeDailyPattern(profile UserProfile, days int) UserProfile {
	hourlyCounts := make(map[int][]int) // è®°å½•æ¯ä¸ªå°æ—¶çš„æŸ¥è¯¢æ¬¡æ•°

	// éå†å†å²æŸ¥è¯¢æ•°æ®
	for timestamp, count := range profile.QueryHistory {
		t, err := time.Parse("2006-01-02 15", timestamp)
		if err == nil {
			hour := t.Hour()
			hourlyCounts[hour] = append(hourlyCounts[hour], count)
		}
	}

	// è®¡ç®—æ¯ä¸ªå°æ—¶çš„å¹³å‡æŸ¥è¯¢æ¬¡æ•°
	for hour, counts := range hourlyCounts {
		total := 0
		for _, c := range counts {
			total += c
		}
		profile.DailyPattern[hour] = float64(total) / float64(len(counts)) // è®¡ç®—å‡å€¼
	}

	return profile
}

// ç§»é™¤è¶…å‡ºæ—¶é—´çª—å£çš„æ•°æ®
func pruneOldHistory(profile UserProfile, windowSize int) UserProfile {
	now := time.Now()
	threshold := now.Add(-time.Duration(windowSize) * time.Hour) // è®¡ç®—çª—å£èµ·ç‚¹

	newHistory := make(map[string]int)
	for timestamp, count := range profile.QueryHistory {
		t, err := time.Parse("2006-01-02 15", timestamp)
		if err == nil && t.After(threshold) {
			newHistory[timestamp] = count
		}
	}

	profile.QueryHistory = newHistory
	return profile
}

// åˆ¤æ–­çŸ­æœŸçƒ­ç”¨æˆ· - æ»‘åŠ¨çª—å£
func (cs *ChunkStorage) IsHotUser(userID string) bool {
	key := []byte(fmt.Sprintf("user_profile:%s", userID))

	data, err := cs.db.Get(key, nil)
	if err != nil {
		return false
	}

	var profile UserProfile
	_ = json.Unmarshal(data, &profile)

	// è®¡ç®—æœ€è¿‘ `windowSize` å°æ—¶å†…çš„æŸ¥è¯¢æ€»æ•°
	windowSize := 24
	totalRecentQueries := 0
	now := time.Now()
	threshold := now.Add(-time.Duration(windowSize) * time.Hour)

	for timestamp, count := range profile.QueryHistory {
		t, err := time.Parse("2006-01-02 15", timestamp)
		if err == nil && t.After(threshold) {
			totalRecentQueries += count
		}
	}

	// å¦‚æœæœ€è¿‘ 24 å°æ—¶å†…æŸ¥è¯¢æ¬¡æ•° >= 10ï¼Œåˆ™åˆ¤å®šä¸ºçƒ­ç”¨æˆ·
	return totalRecentQueries >= UserHotThreshold
}

// åˆ¤æ–­å‘¨æœŸæ€§çƒ­ç”¨æˆ· - SMA
func (cs *ChunkStorage) IsCyclicHotUser(userID string) bool {
	key := []byte(fmt.Sprintf("user_profile:%s", userID))

	data, err := cs.db.Get(key, nil)
	if err != nil {
		return false
	}

	var profile UserProfile
	_ = json.Unmarshal(data, &profile)

	// è·å–å½“å‰å°æ—¶
	currentHour := time.Now().Hour()

	// å¦‚æœå½“å‰å°æ—¶çš„å¹³å‡æŸ¥è¯¢æ¬¡æ•°é«˜äºé˜ˆå€¼ï¼Œåˆ™åˆ¤å®šä¸ºå‘¨æœŸæ€§çƒ­ç”¨æˆ·
	cyclicThreshold := 5.0 // è‡ªå®šä¹‰é˜ˆå€¼
	return profile.DailyPattern[currentHour] >= cyclicThreshold
}

// PreloadUserTransactions é¢„åŠ è½½ç”¨æˆ·äº¤æ˜“ è§¦å‘ Lookahead é¢„åŠ è½½
func (cs *ChunkStorage) PreloadUserTransactions(userID string) {
	key := []byte(fmt.Sprintf("user_profile:%s", userID))

	data, err := cs.db.Get(key, nil)
	if err != nil {
		return
	}

	var profile UserProfile
	_ = json.Unmarshal(data, &profile)

	// **è·å–å½“å‰å°æ—¶**
	currentHour := time.Now().Hour()

	// **å¦‚æœå½“å‰å°æ—¶æ˜¯ç”¨æˆ·çš„å†å²é«˜å³°æ—¶æ®µï¼Œè§¦å‘ Lookahead**
	if profile.DailyPattern[currentHour] >= 5 {
		iter := cs.db.NewIterator(nil, nil)
		defer iter.Release()

		count := 0 // é™åˆ¶æœ€å¤šé¢„åŠ è½½ 10 ç¬”äº¤æ˜“
		for iter.Next() {
			keyStr := string(iter.Key())
			if len(keyStr) > 9 && keyStr[:9] == "tx_index:" {
				txHash := keyStr[9:]
				txData := cs.GetTransaction(txHash)
				if txData != nil {
					cs.txCache.Add(txHash, txData)
					count++
					if count >= 10 { // åªé¢„åŠ è½½æœ€è¿‘ 10 ç¬”äº¤æ˜“
						break
					}
				}
			}
		}
	}
}

// GetTransaction é€šè¿‡äº¤æ˜“å“ˆå¸Œè·å–äº¤æ˜“
func (cs *ChunkStorage) GetTransaction(txHash string) *TxCacheEntry {
	txKey := []byte(fmt.Sprintf("tx_index:%s", txHash))
	chunkIDBytes, err := cs.db.Get(txKey, nil)
	if err != nil {
		return nil
	}
	chunkID, _ := strconv.ParseUint(string(chunkIDBytes), 10, 64)

	chunk, err := cs.loadChunk(chunkID)
	if err != nil {
		return nil
	}

	offset, ok := chunk.TxOffsets[txHash]
	if !ok {
		return nil
	}

	block := chunk.Blocks[offset.BlockNumber]
	tx := block.Transactions[offset.TxIndex]

	return &TxCacheEntry{Tx: &tx, Block: block}
}

// é©¬å°”ç§‘å¤«é“¾é¢„æµ‹
func (cs *ChunkStorage) PredictNextQuery(userID string) string {
	key := []byte(fmt.Sprintf("user_profile:%s", userID))
	data, err := cs.db.Get(key, nil)
	if err != nil {
		return ""
	}

	var profile UserProfile
	_ = json.Unmarshal(data, &profile)

	// **å¦‚æœæŸ¥è¯¢å†å²ä¸è¶³ 3 æ¬¡ï¼Œä¸è¿›è¡Œé¢„æµ‹**
	if len(profile.QuerySequence) < 3 {
		return ""
	}

	// **å–æœ€è¿‘ 2 ç¬”æŸ¥è¯¢**
	lastTx2 := profile.QuerySequence[len(profile.QuerySequence)-2]
	lastTx3 := profile.QuerySequence[len(profile.QuerySequence)-3]

	// **éå† QuerySequenceï¼Œæ‰¾ (Tx3, Tx2) â†’ é¢„æµ‹çš„ TxX**
	for i := 0; i < len(profile.QuerySequence)-3; i++ {
		if profile.QuerySequence[i] == lastTx3 && profile.QuerySequence[i+1] == lastTx2 {
			return profile.QuerySequence[i+2] // é¢„æµ‹çš„ä¸‹ä¸€ä¸ªäº¤æ˜“
		}
	}

	return ""
}
